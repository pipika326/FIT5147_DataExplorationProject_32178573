{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d83b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75828e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pipika/Documents/FIT5147'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentPath = os.getcwd()\n",
    "currentPath "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad99e7",
   "metadata": {},
   "source": [
    "# Daily reports: \n",
    "\n",
    "## check the column names and missing columns\n",
    "Following steps will only read the files ending with `-2020.csv` and will generate all the column names used by those files. \n",
    "\n",
    "With the column names generated, in case some files don't have required column, check if any files need any actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff146eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Province/State',\n",
       "  'Country/Region',\n",
       "  'Last Update',\n",
       "  'Confirmed',\n",
       "  'Deaths',\n",
       "  'Recovered',\n",
       "  'FIPS',\n",
       "  'Admin2',\n",
       "  'Province_State',\n",
       "  'Country_Region',\n",
       "  'Last_Update',\n",
       "  'Lat',\n",
       "  'Long_',\n",
       "  'Active',\n",
       "  'Combined_Key',\n",
       "  'Incidence_Rate',\n",
       "  'Case-Fatality_Ratio',\n",
       "  'Incident_Rate',\n",
       "  'Case_Fatality_Ratio',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 60)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "dailyReports = []       # Store all the file names of sourced daily reports ** WILL USE IT ALL THE TIME **\n",
    "columnNames = []        # Store all the column names of every daily reports\n",
    "\n",
    "# With the column names in step 1\n",
    "# Step 2\n",
    "country = []            # Store all the file names that don't have a 'country/region' column\n",
    "confirmed = []          # Store all the file names that don't have a 'confirmed' column\n",
    "deaths = []             # Store all the file names that don't have a 'deaths' column\n",
    "recovered = []          # Store all the file names that don't have a 'recovered' column\n",
    "active = []             # Store all the file names that don't have a 'active' column\n",
    "\n",
    "\n",
    "# Input folder\n",
    "folderName = 'Datasets/csse_covid_19_daily_reports'\n",
    "folderPath = os.path.join(currentPath, folderName)\n",
    "\n",
    "# Generate a list with all the file names under the directory\n",
    "for curDir, dirList, nameList in os.walk(folderPath, topdown=True):\n",
    "    \n",
    "    # fileName in MM-DD-YYYY.csv\n",
    "    for fileName in nameList:  \n",
    "        \n",
    "        # ----------------------------------------------------------------------------------------------\n",
    "        # Step 1: Saving file names & checking column names\n",
    "        # ----------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Only use 2020.csv data\n",
    "        if fileName.endswith('-2020.csv') is True: \n",
    "            \n",
    "            # Store file names in the list\n",
    "            dailyReports.append(fileName)\n",
    "             \n",
    "            # Read daily report one by one\n",
    "            df = pd.read_csv(os.path.join(curDir, fileName))\n",
    "\n",
    "            # Add unique column names into list\n",
    "            for col in df.columns:\n",
    "                if col not in columnNames:\n",
    "                    columnNames.append(col)  \n",
    "            \n",
    "            # ----------------------------------------------------------------------------------------------\n",
    "            # Step 2: Check missing columns\n",
    "            # ----------------------------------------------------------------------------------------------\n",
    "\n",
    "            if ('Country/Region' not in df.columns) and ('Country_Region' not in df.columns):\n",
    "                country.append(fileName)\n",
    "\n",
    "            if 'Confirmed' not in df.columns:\n",
    "                confirmed.append(fileName)\n",
    "\n",
    "            if 'Deaths' not in df.columns:\n",
    "                deaths.append(fileName)\n",
    "\n",
    "            if 'Recovered' not in df.columns:\n",
    "                recovered.append(fileName)\n",
    "\n",
    "            if 'Active' not in df.columns:\n",
    "                active.append(fileName)\n",
    "\n",
    "columnNames, len(country), len(confirmed), len(deaths), len(recovered), len(active)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb8d94",
   "metadata": {},
   "source": [
    "Data of different state/provinces group by its country will be add up. This project will be looking at country based data. Thus, longitude and latitude cannot be used, it will be combined with other dataset.\n",
    "\n",
    "Case Fatality Ratio not needed for answering Q1, although this concept is needed in Q2, its better to have an ongoing calculation. Will be solved later.\n",
    "\n",
    "Thus, the columns will be used are: \n",
    "- Active\n",
    "- Confirmed\n",
    "- Deaths\n",
    "- Recovered\n",
    "- **Country/Region** / Country_Region\n",
    "\n",
    "Some files missing `active` columns. Active cases is a dependent value, can be calculated and add in the value directly in the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a0287",
   "metadata": {},
   "source": [
    "# Output 1: `/Modified_Datasets/1_cleanedDailyReports`\n",
    "In this step, wrangle the downloaded dataset to a new folder called ***`1_cleanedDailyReports`***. The actions are:\n",
    "1. Only keep files in 2020\n",
    "2. Drop duplicate rows if there is any\n",
    "3. Unified column names: Country/Region / Country_Region to `Country/Region`\n",
    "4. Fill in na value with default value\n",
    "    - 'Confirmed','Recovered','Death'   -> 0\n",
    "    - 'Country/Region'                  -> None\n",
    "5. Add `Active` column if there isn't any\n",
    "    - 'Active' = 'Confirmed'-'Recovered'-'Deaths'\n",
    "6. Drop unrelated columns, only keep these columns:\n",
    "    - Country/Region\n",
    "    - Confirmed\n",
    "    - Deaths\n",
    "    - Recovered\n",
    "    - Active\n",
    "7. Group by 'Country/Region' column and sum other values, save to new dataframe 'group'\n",
    "8. Add `Date` column from its own filename in form 'YYYY-MM-DD'\n",
    "9. Write to csv files with its original file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd72c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognize date value, read date from file name, change form and fill in columns\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# Make an output directory\n",
    "output1Path = os.path.join(currentPath,'Modified_Datasets/1_cleanedDailyReports')\n",
    "if not os.path.exists(output1Path):\n",
    "    os.makedirs(output1Path)\n",
    "\n",
    "# Input folder\n",
    "folderName = 'Datasets/csse_covid_19_daily_reports'\n",
    "folderPath = os.path.join(currentPath, folderName)\n",
    "    \n",
    "# Daily reports in 2020\n",
    "for fileName in dailyReports: \n",
    "\n",
    "    # Read daily report one by one\n",
    "    df = pd.read_csv(os.path.join(folderPath, fileName))\n",
    "    \n",
    "    # Drop duplicate rows \n",
    "    df = df.drop_duplicates()\n",
    "        \n",
    "    # Unified different column names\n",
    "    df = df.rename(columns = {'Country_Region': 'Country/Region'})\n",
    "    \n",
    "    # Add 0 for numeric columns and None for str columns\n",
    "    df[['Confirmed','Recovered','Deaths']] = df[['Confirmed','Recovered','Deaths']].fillna(0)\n",
    "    df[['Country/Region']] = df[['Country/Region']].fillna('None')\n",
    "\n",
    "    # Add 'Active' column if original file don't have one\n",
    "    if 'Active' not in df.columns:\n",
    "        df['Active'] = df['Confirmed'] - df['Recovered'] - df['Deaths'] \n",
    "        \n",
    "    # Select the column wanted\n",
    "    df = df.loc[:, df.columns.isin(['Country/Region', 'Confirmed', 'Deaths', 'Recovered', 'Active'])]\n",
    "    \n",
    "    # Add up all the total cases of each country\n",
    "    group = df.groupby('Country/Region').sum()\n",
    "      \n",
    "    # Add 'Date' column \n",
    "    # Recognize date in MM-DD-YYYY form and shift to 'YYYY-MM-DD' form\n",
    "    reportDate = datetime.strptime(os.path.splitext(fileName)[0],'%m-%d-%Y').date().strftime('%Y-%m-%d')\n",
    "    group['Date'] = reportDate\n",
    "      \n",
    "    # Generate the file path\n",
    "    filePath = os.path.join(output1Path, fileName)\n",
    "    \n",
    "    # Write the file back, keep the index as 'country' recognized as index\n",
    "    group.to_csv(filePath, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c123e8",
   "metadata": {},
   "source": [
    "# Combine with geographical coordinate dataset\n",
    "\n",
    "## Check for country names in all the cleaned files in `1_cleanedDailyReports`\n",
    "Generate a unique country name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b3fb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input folder\n",
    "folderName = 'Modified_Datasets/1_cleanedDailyReports'\n",
    "folderPath = os.path.join(currentPath, folderName)\n",
    "\n",
    "countryName = []        # Store all the file names of sourced daily reports \n",
    "\n",
    "for fileName in dailyReports:\n",
    "\n",
    "    # Read daily report one by one\n",
    "    df = pd.read_csv(os.path.join(folderPath, fileName))\n",
    "    \n",
    "    # Add country names into list\n",
    "    for country in df['Country/Region']:\n",
    "        if country not in countryName:\n",
    "            countryName.append(country) \n",
    "countryName.sort()\n",
    "len(countryName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448cceb",
   "metadata": {},
   "source": [
    "## Read `countries.csv` and check for unmatching naming\n",
    "A `geo` list was initialized which will be act like a data frame structure, will be storing all the rows that will be put in the final `geoCoor` data frame. \n",
    "\n",
    "With unmatching country naming, will be resolve in two different ways:\n",
    "- With the different calling of country names in the downloaded dataset, change and unified them within the file (read, change, write back to `1_cleanedDailyReports` folder.\n",
    "    - Check matching country names directly `==` -> locate df with condition, `.tolist()` save rows to list -> append rows in `geo` (column:'id', 'name', 'latitude', 'longitude', 'native')\n",
    "    - Check if match the native writing in rows `in` `geo[i][(native)4]` -> pair of name will be unify in the next step\n",
    "    - Split the country names in to key words, excluding the connecting words ('and','of'), check for matching words `in` `geo[i][1(name)]` -> obvious matching pairs will unify in next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f0b3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input folder & Output folder\n",
    "folderName = 'Modified_Datasets/1_cleanedDailyReports'\n",
    "folderPath = os.path.join(currentPath, folderName)\n",
    "\n",
    "# A set of matching country names\n",
    "change = ['Cabo Verde','Timor-Leste',' Azerbaijan','Mainland China','Iran (Islamic Republic of)','Viet Nam',\n",
    "          'Republic of Ireland','Republic of Moldova','Korea, South','Saint Martin','US','UK','Taiwan*',\n",
    "          'Taipei and environs','Bahamas, The','Bahamas','Gambia, The','Hong Kong SAR','Macao SAR','Macao',\n",
    "          \"Cote d'Ivoire\",'Holy See','Gambia','Republic of the Congo','West Bank and Gaza','Russian Federation','Czechia']\n",
    "to = ['Cape Verde','East Timor','Azerbaijan','China','Iran','Vietnam',\n",
    "      'Ireland','Moldova','South Korea','St. Martin','United States','United Kingdom','Taiwan',\n",
    "      'Taiwan','The Bahamas','The Bahamas','The Gambia','Hong Kong','Macau','Macau',\n",
    "      'Ivory Coast','Vatican City','The Gambia','Congo (Brazzaville)','Palestine','Russia','Czech Republic']\n",
    "\n",
    "countryName = []        # Store all the file names of sourced daily reports\n",
    "for fileName in dailyReports:\n",
    "\n",
    "    # Read daily report one by one\n",
    "    df = pd.read_csv(os.path.join(folderPath, fileName))\n",
    "    \n",
    "    # Replace the different country names with only one form\n",
    "    df['Country/Region'] = df['Country/Region'].replace(change, to)\n",
    "    \n",
    "    # Write the file back\n",
    "    df.to_csv(os.path.join(folderPath, fileName), index = False)\n",
    "    \n",
    "    # Check that again\n",
    "    for country in df['Country/Region']:\n",
    "        if country not in countryName:\n",
    "            countryName.append(country) \n",
    "            \n",
    "countryName.sort()\n",
    "len(countryName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c874db26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search words:  ['Channel Islands', 'Cayman Islands']\n",
      "search words:  ['Channel Islands', 'Faroe Islands']\n",
      "search words:  ['Channel Islands', 'Marshall Islands']\n",
      "search words:  ['Channel Islands', 'Solomon Islands']\n",
      "search words:  ['North Ireland', 'Ireland']\n",
      "search words:  ['Papua New Guinea', 'New Zealand']\n",
      "search words:  ['Papua New Guinea', 'Equatorial Guinea']\n",
      "search words:  ['Papua New Guinea', 'Guinea']\n",
      "search words:  ['Papua New Guinea', 'Guinea-Bissau']\n",
      "search words:  ['Republic of Korea', 'Central African Republic']\n",
      "search words:  ['Republic of Korea', 'Czech Republic']\n",
      "search words:  ['Republic of Korea', 'Dominican Republic']\n",
      "search words:  ['Saint Barthelemy', 'Saint Lucia']\n",
      "search words:  ['Saint Kitts and Nevis', 'Saint Lucia']\n",
      "search words:  ['Saint Vincent and the Grenadines', 'Saint Lucia']\n",
      "search words:  ['South Korea', 'South Africa']\n",
      "search words:  ['South Korea', 'South Sudan']\n",
      "search words:  ['St. Martin', 'Martinique']\n"
     ]
    }
   ],
   "source": [
    "# Create 'countries' data frame storing long and lat values\n",
    "\n",
    "# Input file\n",
    "fileName = 'countries.csv'\n",
    "filePath = os.path.join(currentPath, 'Datasets', fileName)\n",
    "df = pd.read_csv(filePath)\n",
    "\n",
    "from re import search\n",
    "\n",
    "geo = []                # A list storing all the row lists, will transform into data frame structure\n",
    "unmatch = []            # Store all the country names (from daily reports) that not in (matching) countries.csv \n",
    "\n",
    "# Save matching country to geo list first\n",
    "for cName in countryName:\n",
    "    if cName in df.name.values:\n",
    "        row = df.loc[df.name.values == cName, ['id', 'name', 'latitude', 'longitude', 'native']].values.tolist()\n",
    "        geo.append(row[0])\n",
    "    else:\n",
    "        unmatch.append(cName)\n",
    "\n",
    "## Find out the naming difference within the countryName list, if found, change in files\n",
    "\n",
    "# Check for native country name writing, add to 'change' and 'to' in the last step\n",
    "for cName in unmatch:\n",
    "    for i in range(len(geo)):\n",
    "        if cName in geo[i][4]:\n",
    "            print('native writing: ',[cName,geo[i][1]])\n",
    "            unmatch.remove(cName)\n",
    "\n",
    "# Split names in to single words, rough finding, add obvious matching to 'change' and 'to' in the last step\n",
    "for cName in unmatch:\n",
    "    split = cName.split()\n",
    "    for words in split:\n",
    "        for i in range(len(geo)):\n",
    "            if words in geo[i][1] and words not in ['and', 'of']:\n",
    "                print('search words: ',[cName,geo[i][1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44263b3c",
   "metadata": {},
   "source": [
    "With matching names found, unified them within its own files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae2254",
   "metadata": {},
   "source": [
    "- With the different calling between two datasets (Daily reports and countries.csv), change the country name in the data frame read from countries.csv to match the naming in daily reports.\n",
    "    - Search ignoring cases, add row in `geo` with the country name from daily reports at second position of row\n",
    "    - Search with regular expression using `.contains()`, ignoring cases \n",
    "        -Some country names need to go back to the last step and change within files\n",
    "    - Split names in to words, excluding ['the','of','islands','and','republic','west'], search with `.contains()`, but only print if the output has only 1 result\n",
    "    - With some other naming of the country names, googled and unified mannually\n",
    "\n",
    "Finally, transform list into data frame. Add 'North Ireland' as not included in countries.csv. Ignore the cruise ship related data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae184972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore cases:  ['Antigua and Barbuda', [[10, 'Antigua And Barbuda', 17.05, -61.8, 'Antigua and Barbuda']]]\n",
      "Ignore cases:  ['Papua New Guinea', [[171, 'Papua new Guinea', -6.0, 147.0, 'Papua Niugini']]]\n",
      "Ignore cases:  ['Saint Kitts and Nevis', [[185, 'Saint Kitts And Nevis', 17.33333333, -62.75, 'Saint Kitts and Nevis']]]\n",
      "Ignore cases:  ['Trinidad and Tobago', [[223, 'Trinidad And Tobago', 11.0, -61.0, 'Trinidad and Tobago']]]\n",
      "contains:  ['Burma', []]\n",
      "contains:  ['Channel Islands', []]\n",
      "contains:  ['Congo (Brazzaville)', []]\n",
      "contains:  ['Congo (Kinshasa)', []]\n",
      "contains:  ['Croatia', [[55, 'Croatia (Hrvatska)', 45.16666666, 15.5, 'Hrvatska']]]\n",
      "contains:  ['Curacao', []]\n",
      "contains:  ['Diamond Princess', []]\n",
      "contains:  ['Eswatini', []]\n",
      "contains:  ['Fiji', [[73, 'Fiji Islands', -18.0, 175.0, 'Fiji']]]\n",
      "contains:  ['Hong Kong', [[98, 'Hong Kong S.A.R.', 22.25, 114.16666666, '香港']]]\n",
      "contains:  ['MS Zaandam', []]\n",
      "contains:  ['Macau', [[128, 'Macau S.A.R.', 22.16666666, 113.55, '澳門']]]\n",
      "contains:  ['North Ireland', []]\n",
      "contains:  ['North Macedonia', []]\n",
      "contains:  ['Others', []]\n",
      "contains:  ['Palestine', []]\n",
      "contains:  ['Republic of Korea', []]\n",
      "contains:  ['Saint Barthelemy', []]\n",
      "contains:  ['Saint Vincent and the Grenadines', [[188, 'Saint Vincent And The Grenadines', 13.25, -61.2, 'Saint Vincent and the Grenadines']]]\n",
      "contains:  ['St. Martin', []]\n",
      "contains:  ['The Bahamas', []]\n",
      "contains:  ['The Gambia', []]\n",
      "contains:  ['Vatican City', [[238, 'Vatican City State (Holy See)', 41.9, 12.45, 'Vaticano']]]\n",
      "word= guernsey split, contains:  ['Guernsey', [[91, 'Guernsey and Alderney', 49.46666666, -2.58333333, 'Guernsey']]]\n",
      "------ ['Guernsey', [91, 'Guernsey', 49.46666666, -2.58333333, 'Guernsey']]\n",
      "word= netherlands split, contains:  ['Netherlands', [[156, 'Netherlands The', 52.5, 5.75, 'Nederland']]]\n",
      "------ ['Netherlands', [156, 'Netherlands', 52.5, 5.75, 'Nederland']]\n",
      "word= macedonia split, contains:  ['North Macedonia', [[129, 'Macedonia', 41.83333333, 22.0, 'Северна Македонија']]]\n",
      "------ ['North Macedonia', [129, 'North Macedonia', 41.83333333, 22.0, 'Северна Македонија']]\n",
      "word= barthelemy split, contains:  ['Saint Barthelemy', [[189, 'Saint-Barthelemy', 18.5, -63.41666666, 'Saint-Barthélemy']]]\n",
      "------ ['Saint Barthelemy', [189, 'Saint Barthelemy', 18.5, -63.41666666, 'Saint-Barthélemy']]\n",
      "word= bahamas split, contains:  ['The Bahamas', [[17, 'Bahamas The', 24.25, -76.0, 'Bahamas']]]\n",
      "------ ['The Bahamas', [17, 'The Bahamas', 24.25, -76.0, 'Bahamas']]\n",
      "word= occupied split, contains:  ['occupied Palestinian territory', [[169, 'Palestinian Territory Occupied', 31.9, 35.2, 'فلسطين']]]\n",
      "------ ['occupied Palestinian territory', [169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين']]\n",
      "word= palestinian split, contains:  ['occupied Palestinian territory', [[169, 'Palestinian Territory Occupied', 31.9, 35.2, 'فلسطين']]]\n",
      "------ ['occupied Palestinian territory', [169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين']]\n",
      "[58, 'Czech Republic', 49.75, 15.5, 'Česká republika'] [58, 'Czechia', 49.75, 15.5, 'Česká republika']\n",
      "[169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين'] [169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين']\n",
      "[169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين'] [169, 'Palestine', 31.9, 35.2, 'فلسطين']\n",
      "[169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين'] [169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين']\n",
      "[169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين'] [169, 'Palestine', 31.9, 35.2, 'فلسطين']\n",
      "[58, 'Czechia', 49.75, 15.5, 'Česká republika'] [58, 'Czech Republic', 49.75, 15.5, 'Česká republika']\n",
      "[169, 'Palestine', 31.9, 35.2, 'فلسطين'] [169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين']\n",
      "[169, 'Palestine', 31.9, 35.2, 'فلسطين'] [169, 'occupied Palestinian territory', 31.9, 35.2, 'فلسطين']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pipika/opt/anaconda3/lib/python3.8/site-packages/pandas/core/strings/accessor.py:101: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angola</td>\n",
       "      <td>-12.500000</td>\n",
       "      <td>18.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Kosovo</td>\n",
       "      <td>42.561291</td>\n",
       "      <td>20.340304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Curacao</td>\n",
       "      <td>12.116667</td>\n",
       "      <td>-68.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>North Ireland</td>\n",
       "      <td>54.787700</td>\n",
       "      <td>-6.492300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country/Region   Latitude  Longitude\n",
       "0      Afghanistan  33.000000  65.000000\n",
       "1          Albania  41.000000  20.000000\n",
       "2          Algeria  28.000000   3.000000\n",
       "3          Andorra  42.500000   1.500000\n",
       "4           Angola -12.500000  18.500000\n",
       "..             ...        ...        ...\n",
       "206         Zambia -15.000000  30.000000\n",
       "207       Zimbabwe -20.000000  30.000000\n",
       "208         Kosovo  42.561291  20.340304\n",
       "209        Curacao  12.116667 -68.933333\n",
       "210  North Ireland  54.787700  -6.492300\n",
       "\n",
       "[211 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rough search in df, if found, change in df\n",
    "\n",
    "# Ignore cases\n",
    "\n",
    "for cName in unmatch:\n",
    "    if cName.lower() in df.name.str.lower().values:\n",
    "        row = df.loc[df.name.str.lower().values == cName.lower(), ['id', 'name', 'latitude', 'longitude', 'native']].values.tolist()\n",
    "        print('Ignore cases: ',[cName, row])\n",
    "        row[0][1] = cName\n",
    "        geo.append(row[0])\n",
    "        unmatch.remove(cName)\n",
    "\n",
    "# Use .contains() to search with re, ignore case, match the naming in map to the WHO report\n",
    "\n",
    "for cName in unmatch:\n",
    "    row = df.loc[df.name.str.contains(cName, flags=re.I, regex=True),['id', 'name', 'latitude', 'longitude', 'native']].values.tolist()\n",
    "    print('contains: ',[cName,row])\n",
    "    # only write in the row with matching output\n",
    "    if len(row) != 0:\n",
    "        row[0][1] = cName\n",
    "        geo.append(row[0])\n",
    "        unmatch.remove(cName)\n",
    "\n",
    "# Found that 'UK' & 'US' are missleading the .contains function, change to long form in last step\n",
    "# Also found some countries with two/three different names, unified them in last step\n",
    "    \n",
    "    \n",
    "# Split names in to single words, rough finding\n",
    "\n",
    "for cName in unmatch:\n",
    "    split = cName.lower().split()\n",
    "    for words in split:\n",
    "        if words not in ['the','of','islands','and','republic','west']:\n",
    "            row = df.loc[df.name.str.contains(words, flags=re.I, regex=True),['id', 'name', 'latitude', 'longitude', 'native']].values.tolist()\n",
    "            if len(row) == 1:\n",
    "                print('word=',words,'split, contains: ',[cName,row])\n",
    "                \n",
    "                # only write in the row with matching output\n",
    "                row[0][1] = cName\n",
    "                print('------',[cName,row[0]])\n",
    "                geo.append(row[0])\n",
    "                if cName in unmatch:\n",
    "                    unmatch.remove(cName)\n",
    "\n",
    "# Match name manully\n",
    "\n",
    "tuples = [('Czech','Czechia'),('Myanmar','Burma'),('Swaziland','Eswatini'),('Korea South','South Korea'),\n",
    "          ('Korea North','Republic of Korea'),('Gambia','The Gambia'),\n",
    "          ('Palestin','Palestine'),('Martin','St. Martin'),('Cura','Curacao'),('Ivory','Ivory Coast')]\n",
    "\n",
    "for t in tuples:\n",
    "    row = df.loc[df.name.str.contains(t[0], flags=re.I, regex=True),['id', 'name', 'latitude', 'longitude', 'native']].values.tolist()\n",
    "    # only write in the row with matching output\n",
    "    if len(row) == 1:\n",
    "        row[0][1] = t[1]\n",
    "        geo.append(row[0])\n",
    "        if t[1] in unmatch:\n",
    "            unmatch.remove(t[1])   \n",
    "\n",
    "df.name = df.name.replace(['Congo The Democratic Republic Of The','Congo'],['Congo (Kinshasa)','Congo (Brazzaville)'])\n",
    "row = df.loc[df.name.values == 'Congo (Kinshasa)', ['id', 'name', 'latitude', 'longitude', 'native']].values.tolist()\n",
    "geo.append(row[0])\n",
    "unmatch.remove('Congo (Kinshasa)')\n",
    "row = df.loc[df.name.values == 'Congo (Brazzaville)', ['id', 'name', 'latitude', 'longitude', 'native']].values.tolist()\n",
    "geo.append(row[0])\n",
    "unmatch.remove('Congo (Brazzaville)')\n",
    "\n",
    "## Find repeating data\n",
    "\n",
    "for i in range(len(geo)):\n",
    "    for j in range(len(geo)):\n",
    "        if i != j and geo[i][0] == geo[j][0]:\n",
    "            print(geo[i],geo[j])\n",
    "geo.sort()  \n",
    "\n",
    "geoCoor = pd.DataFrame(geo, columns=['id','Country/Region','Latitude', 'Longitude','native'])\n",
    "geoCoor = geoCoor.drop_duplicates()\n",
    "geoCoor = geoCoor.loc[:, geoCoor.columns.isin(['Country/Region','Latitude', 'Longitude'])]\n",
    "\n",
    "\n",
    "nIreland = {'Country/Region':'North Ireland','Latitude':54.7877, 'Longitude':-6.4923}\n",
    "geoCoor = geoCoor.append(nIreland, ignore_index=True)\n",
    "geoCoor\n",
    "\n",
    "# # Only left cruise ship related:  ['Cruise Ship', 'Diamond Princess', 'Channel Islands','Others','MS Zaandam']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe511c6",
   "metadata": {},
   "source": [
    "# Output 2: `Modified_Datasets/2_combineLatLong`\n",
    "\n",
    "Combine the files in `Output 1` with longitude and latitude columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c407d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import date, datetime, timedelta\n",
    "\n",
    "# # List of datetime.date type of data in YYYY-MM-DD form\n",
    "# yyyymmdd = []\n",
    "\n",
    "# # fileName in MM-DD-YYYY\n",
    "# for fileName in dailyReports:  \n",
    "\n",
    "#     # Recgnize date, now in form YYYY-MM-DD, type:datetime.date\n",
    "#     # datetime.strptime(strdate, 'format') -> yyyy mm dd hh min ss\n",
    "#     # .date() to only keep 'yyyy mm dd'\n",
    "#     ymd = datetime.strptime(os.path.splitext(fileName)[0],'%m-%d-%Y').date()\n",
    "#     yyyymmdd.append(ymd)\n",
    "\n",
    "\n",
    "# # A list of filename for every 5 days since 2020-01-22\n",
    "# every5day = []\n",
    "\n",
    "# # From the earlies file, get the date for every 5 days\n",
    "# start_date = min(yyyymmdd)\n",
    "# while start_date <= max(yyyymmdd):\n",
    "    \n",
    "#     # Save back to the filename form (datetime->str + .csv)\n",
    "#     # datatime.strftime('format'): datetime->str\n",
    "#     mdy = start_date.strftime('%m-%d-%Y')\n",
    "#     fileName = mdy + '.csv'\n",
    "#     every5day.append(fileName)\n",
    "\n",
    "#     # Add 5 days to the date for next loop\n",
    "#     start_date += timedelta(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f004fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an output directory\n",
    "output2Path = os.path.join(currentPath,'Modified_Datasets/2_combineLatLong')\n",
    "if not os.path.exists(output2Path):\n",
    "    os.makedirs(output2Path)\n",
    "\n",
    "# Input folder\n",
    "folderName = 'Modified_Datasets/1_cleanedDailyReports'\n",
    "folderPath = os.path.join(currentPath, folderName)\n",
    "\n",
    "for fileName in dailyReports: \n",
    "\n",
    "    # Generate the file path\n",
    "    filePath = os.path.join(folderPath, fileName)\n",
    "\n",
    "    # Read daily report one by one\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    # left merge: keep all other columns in df, only add lat long in\n",
    "    df = pd.merge(df, geoCoor, how='left', on=['Country/Region'])\n",
    "    \n",
    "    # Generate the file path\n",
    "    filePath = os.path.join(output2Path, fileName)\n",
    "    \n",
    "    # Write the file back\n",
    "    df.to_csv(filePath, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d771c9",
   "metadata": {},
   "source": [
    "# Lock down dates `countryLockdowndatesJHUMatch.csv`\n",
    "\n",
    "Read into df and only keep the 'Country/Region','Date'. For the countries that do not have a lock down date, only keep the rows that 'Date' column is `.notna()`. \n",
    "\n",
    "Check if there are unmatching country names. `.replace()` column values with matching names.\n",
    "\n",
    "Save df into `lockdown` dataframe, add two more column showing the date 14 an 28 days after lockdown date by changing 'LockdownDate' column to a datetime value using `pd.to_datetime('column',format='%Y-%m-%d')+timedelta(14/28)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1bbd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>LockdownDate</th>\n",
       "      <th>14 Days</th>\n",
       "      <th>28 Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>2020-02-06</td>\n",
       "      <td>2020-02-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>2020-04-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Macau</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>2020-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taiwan</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>2020-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>2020-04-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Montenegro</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Kyrgyzstan</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Mauritius</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>2020-04-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Uganda</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-04-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Mali</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-04-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country/Region LockdownDate    14 Days    28 Days\n",
       "0            China   2020-01-23 2020-02-06 2020-02-20\n",
       "1        Hong Kong   2020-03-23 2020-04-06 2020-04-20\n",
       "2            Macau   2020-01-26 2020-02-09 2020-02-23\n",
       "3           Taiwan   2020-02-02 2020-02-16 2020-03-01\n",
       "4    United States   2020-03-23 2020-04-06 2020-04-20\n",
       "..             ...          ...        ...        ...\n",
       "168     Montenegro   2020-03-24 2020-04-07 2020-04-21\n",
       "170     Kyrgyzstan   2020-03-24 2020-04-07 2020-04-21\n",
       "171      Mauritius   2020-03-17 2020-03-31 2020-04-14\n",
       "187         Uganda   2020-03-18 2020-04-01 2020-04-15\n",
       "197           Mali   2020-03-18 2020-04-01 2020-04-15\n",
       "\n",
       "[134 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileName = 'countryLockdowndatesJHUMatch.csv'\n",
    "filePath = os.path.join(currentPath, 'Datasets', fileName)\n",
    "df = pd.read_csv(filePath)\n",
    "df = df.loc[:,['Country/Region','Date']]\n",
    "df = df.drop_duplicates()\n",
    "df = df[df['Date'].notna()]\n",
    "df\n",
    "countryName\n",
    "# # unmatching country names\n",
    "# for name in df['Country/Region'].values:\n",
    "#     if name not in countryName:\n",
    "#         print(name)\n",
    "\n",
    "df['Country/Region'] = df['Country/Region'].replace(['Mainland China','US','UK','Republic of Ireland'],\n",
    "                                                    ['China','United States','United Kingdom','Ireland'])\n",
    "# unmatching country names\n",
    "for name in df['Country/Region'].values:\n",
    "    if name not in countryName:\n",
    "        print(name)\n",
    "        \n",
    "lockdown = pd.DataFrame()\n",
    "lockdown[['Country/Region','LockdownDate']] = df[['Country/Region','Date']]\n",
    "lockdown['LockdownDate'] = pd.to_datetime(lockdown['LockdownDate'], format = '%Y-%m-%d')\n",
    "lockdown['14 Days'] = pd.to_datetime(lockdown['LockdownDate'], format = '%Y-%m-%d') + timedelta(14)\n",
    "lockdown['28 Days'] = pd.to_datetime(lockdown['LockdownDate'], format = '%Y-%m-%d') + timedelta(28)\n",
    "lockdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d252412",
   "metadata": {},
   "source": [
    "# Combine all files from `'Modified_Datasets/2_combineLatLong'`\n",
    "Use `pd.concat()` to combine all the df read from input folder. In order to sort the column by 'Country/Region','Date' columns, change the 'Date' column to a `datetime` value in order to sort in ascending order. The `concat_df` will sort by country first and then date of the recorded cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c9a6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input folder    \n",
    "folderName = 'Modified_Datasets/2_combineLatLong'\n",
    "all_df = (pd.read_csv(os.path.join(currentPath, folderName, fileName)) for fileName in dailyReports)\n",
    "concat_df = pd.concat(all_df, ignore_index=True)\n",
    "\n",
    "concat_df['Date'] = pd.to_datetime(concat_df['Date'], format='%Y-%m-%d')\n",
    "concat_df = concat_df.sort_values(['Country/Region','Date'], ascending = [1,1])\n",
    "concat_df = concat_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3b489",
   "metadata": {},
   "source": [
    "# Final output: `Modified_Datasets/3_combinedAllDates`\n",
    "Merge the combined dataset and lockdown together on 'country' column, keeping all the values in combined dataset (left merge). Added three boolean columns show if that date that country had locked down or not by comparing the value between date of the cases been recorded and lockdown dates. Save to 'COVID19_Dataset.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2194a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an output directory\n",
    "output3Path = os.path.join(currentPath,'Modified_Datasets/3_combinedAllDates')\n",
    "if not os.path.exists(output3Path):\n",
    "    os.makedirs(output3Path)\n",
    "\n",
    "df = pd.merge(concat_df, lockdown, how='left', on=['Country/Region'])\n",
    "# df['LockdownDate'] = pd.to_datetime(df['LockdownDate'], format = '%Y-%m-%d')\n",
    "df.loc[df['Date'] >= df['LockdownDate'],['LockdownStatus']] = True\n",
    "df.loc[df['Date'] < df['LockdownDate'],['LockdownStatus']] = False\n",
    "df.loc[df['Date'] >= df['14 Days'],['14d_checkpoint']] = True\n",
    "df.loc[df['Date'] < df['14 Days'],['14d_checkpoint']] = False\n",
    "df.loc[df['Date'] >= df['28 Days'],['28d_checkpoint']] = True\n",
    "df.loc[df['Date'] < df['28 Days'],['28d_checkpoint']] = False\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv(os.path.join(output3Path, 'COVID19_Dataset.csv'), index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175818ed",
   "metadata": {},
   "source": [
    "Lastly, group the df by 'country' names, cumulate the cases from the first date and add in cumulate columns. And write to 'COVID19_DatasetCum.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e8f8a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-36d366a46fd0>:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df[['Confirmed_Cum', 'Deaths_Cum', 'Recovered_Cum', 'Active_Cum']]= df.groupby(['Country/Region'])['Confirmed', 'Deaths', 'Recovered', 'Active'].cumsum()\n"
     ]
    }
   ],
   "source": [
    "df[['Confirmed_Cum', 'Deaths_Cum', 'Recovered_Cum', 'Active_Cum']]= df.groupby(['Country/Region'])['Confirmed', 'Deaths', 'Recovered', 'Active'].cumsum()\n",
    "df.to_csv(os.path.join(output3Path, 'COVID19_DatasetCum.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c877d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
